{"iteration": 1, "episodes": 8, "train/loss": 0.0, "reward/mean": -0.25, "eval/mean_reward": -0.32, "eval/win_rate": 0.34}
{"iteration": 1, "episodes": 4, "train/loss": 0.0, "reward/mean": -0.5, "eval/mean_reward": 0.0, "eval/win_rate": 0.5}
{"iteration": 2, "episodes": 4, "train/loss": 0.0, "reward/mean": -0.5, "eval/mean_reward": 0.0, "eval/win_rate": 0.5}
{"iteration": 3, "episodes": 4, "train/loss": 0.0, "reward/mean": 0.5, "eval/mean_reward": 0.0, "eval/win_rate": 0.5}
{"iteration": 1, "episodes": 8, "train/loss": 0.374999463558197, "reward/mean": -0.25, "eval/mean_reward": 0.0, "eval/win_rate": 0.5}
{"iteration": 2, "episodes": 8, "train/loss": 0.5999997854232788, "reward/mean": 0.5, "eval/mean_reward": -0.6, "eval/win_rate": 0.2}
{"iteration": 3, "episodes": 8, "train/loss": 0.29999983310699463, "reward/mean": -0.25, "eval/mean_reward": -0.4, "eval/win_rate": 0.3}
{"iteration": 1, "episodes": 8, "train/loss": 0.374999463558197, "reward/mean": -0.25, "eval/mean_reward": -0.22, "eval/win_rate": 0.39}
{"iteration": 2, "episodes": 8, "train/loss": 0.14999914169311523, "reward/mean": -0.75, "eval/mean_reward": -0.28, "eval/win_rate": 0.36}
{"iteration": 3, "episodes": 8, "train/loss": 0.37499791383743286, "reward/mean": -0.25, "eval/mean_reward": -0.18, "eval/win_rate": 0.41}
{"iteration": 4, "episodes": 8, "train/loss": 0.35555416345596313, "reward/mean": 0.0, "eval/mean_reward": -0.3, "eval/win_rate": 0.35}
{"iteration": 5, "episodes": 8, "train/loss": 0.12499989569187164, "reward/mean": -0.75, "eval/mean_reward": -0.32, "eval/win_rate": 0.34}
{"iteration": 6, "episodes": 8, "train/loss": 0.2222193032503128, "reward/mean": -0.5, "eval/mean_reward": -0.3, "eval/win_rate": 0.35}
{"iteration": 7, "episodes": 8, "train/loss": 0.7777775526046753, "reward/mean": 0.25, "eval/mean_reward": -0.2, "eval/win_rate": 0.4}
{"iteration": 8, "episodes": 8, "train/loss": 0.39999857544898987, "reward/mean": 0.0, "eval/mean_reward": -0.26, "eval/win_rate": 0.37}
{"iteration": 9, "episodes": 8, "train/loss": 0.4000000059604645, "reward/mean": -0.5, "eval/mean_reward": -0.22, "eval/win_rate": 0.39}
{"iteration": 10, "episodes": 8, "train/loss": 0.3999978303909302, "reward/mean": -0.25, "eval/mean_reward": -0.24, "eval/win_rate": 0.38}
{"iteration": 1, "episodes": 8, "train/loss": 0.18277132511138916, "reward/mean": -0.25, "eval/mean_reward": -0.22, "eval/win_rate": 0.39}
{"iteration": 2, "episodes": 8, "train/loss": 0.2775755226612091, "reward/mean": -0.5, "eval/mean_reward": -0.32, "eval/win_rate": 0.34}
{"iteration": 1, "episodes": 4, "train/loss": 0.0, "reward/mean": -0.5, "eval/mean_reward": -0.2, "eval/win_rate": 0.4}
{"iteration": 2, "episodes": 4, "train/loss": 0.0, "reward/mean": -1.0, "eval/mean_reward": 0.2, "eval/win_rate": 0.6}
{"iteration": 1, "episodes": 8, "train/loss": -0.1300441473722458, "reward/mean": -0.25, "eval/mean_reward": -0.2, "eval/win_rate": 0.4}
{"iteration": 1, "episodes": 5, "train/loss": 5.433863639831543, "train/policy_loss": 2.255589246749878, "train/value_loss": 6.356549263000488, "reward/mean": -0.6, "eval/mean_reward": -0.6, "eval/win_rate": 0.2}
{"iteration": 2, "episodes": 5, "train/loss": 2.822596549987793, "train/policy_loss": 1.3729697465896606, "train/value_loss": 2.8992533683776855, "reward/mean": 0.6, "eval/mean_reward": -0.6, "eval/win_rate": 0.2}
{"iteration": 3, "episodes": 5, "train/loss": 2.2899744510650635, "train/policy_loss": 1.1776039600372314, "train/value_loss": 2.224740982055664, "reward/mean": 0.2, "eval/mean_reward": -0.2, "eval/win_rate": 0.4}
{"iteration": 4, "episodes": 5, "train/loss": 1.8468804359436035, "train/policy_loss": 1.0074877738952637, "train/value_loss": 1.6787853240966797, "reward/mean": 0.2, "eval/mean_reward": 0.2, "eval/win_rate": 0.6}
{"iteration": 5, "episodes": 5, "train/loss": 2.9199612140655518, "train/policy_loss": 1.778320074081421, "train/value_loss": 2.2832822799682617, "reward/mean": -0.2, "eval/mean_reward": -1.0, "eval/win_rate": 0.0}
{"iteration": 1, "episodes": 5, "train/loss": 5.433863639831543, "train/policy_loss": 2.255589246749878, "train/value_loss": 6.356549263000488, "reward/mean": -0.6, "eval/mean_reward": -0.6, "eval/win_rate": 0.2}
{"iteration": 2, "episodes": 5, "train/loss": 2.822596549987793, "train/policy_loss": 1.3729697465896606, "train/value_loss": 2.8992533683776855, "reward/mean": 0.6, "eval/mean_reward": -0.6, "eval/win_rate": 0.2}
{"iteration": 3, "episodes": 5, "train/loss": 2.2899744510650635, "train/policy_loss": 1.1776039600372314, "train/value_loss": 2.224740982055664, "reward/mean": 0.2, "eval/mean_reward": -0.2, "eval/win_rate": 0.4}
{"iteration": 4, "episodes": 5, "train/loss": 1.8468804359436035, "train/policy_loss": 1.0074877738952637, "train/value_loss": 1.6787853240966797, "reward/mean": 0.2, "eval/mean_reward": 0.2, "eval/win_rate": 0.6}
{"iteration": 5, "episodes": 5, "train/loss": 2.9199612140655518, "train/policy_loss": 1.778320074081421, "train/value_loss": 2.2832822799682617, "reward/mean": -0.2, "eval/mean_reward": -1.0, "eval/win_rate": 0.0}
{"iteration": 6, "episodes": 5, "train/loss": 0.7148361206054688, "train/policy_loss": 0.41370928287506104, "train/value_loss": 0.6022537350654602, "reward/mean": 0.6, "eval/mean_reward": -0.6, "eval/win_rate": 0.2}
{"iteration": 7, "episodes": 5, "train/loss": 1.203734040260315, "train/policy_loss": 0.8209937810897827, "train/value_loss": 0.7654804587364197, "reward/mean": -0.6, "eval/mean_reward": -0.6, "eval/win_rate": 0.2}
{"iteration": 8, "episodes": 5, "train/loss": 1.9560575485229492, "train/policy_loss": 1.49280846118927, "train/value_loss": 0.9264981746673584, "reward/mean": 0.2, "eval/mean_reward": -1.0, "eval/win_rate": 0.0}
{"iteration": 9, "episodes": 5, "train/loss": 1.0845463275909424, "train/policy_loss": 0.5519399046897888, "train/value_loss": 1.0652129650115967, "reward/mean": 0.2, "eval/mean_reward": 0.2, "eval/win_rate": 0.6}
{"iteration": 10, "episodes": 5, "train/loss": 1.7395093441009521, "train/policy_loss": 1.0198365449905396, "train/value_loss": 1.4393455982208252, "reward/mean": -0.2, "eval/mean_reward": -0.2, "eval/win_rate": 0.4}
{"iteration": 1, "episodes": 10, "train/loss": 5.226456165313721, "train/policy_loss": 3.1422877311706543, "train/value_loss": 4.168336868286133, "reward/mean": -0.2, "eval/mean_reward": -0.4, "eval/win_rate": 0.3}
{"iteration": 1, "episodes": 10, "train/loss": 3.8859872817993164, "train/policy_loss": 1.8018187284469604, "train/value_loss": 4.168336868286133, "reward/mean": -0.2, "eval/mean_reward": -0.6, "eval/win_rate": 0.2}
